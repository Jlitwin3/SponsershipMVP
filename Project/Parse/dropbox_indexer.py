import os
import dropbox
import fitz  # PyMuPDF
from io import BytesIO

# Initialize Dropbox Client
DROPBOX_ACCESS_TOKEN = os.getenv("DROPBOX_ACCESS_TOKEN")
if not DROPBOX_ACCESS_TOKEN:
    # We don't raise error here to allow module import, but check later
    pass

dbx = dropbox.Dropbox(DROPBOX_ACCESS_TOKEN) if DROPBOX_ACCESS_TOKEN else None
DROPBOX_FOLDER = "/L‚Äômu-Oa (Sports Sponsorship AI Project)"

def fetch_dropbox_pdfs(folder_path=DROPBOX_FOLDER):
    """Get list of PDFs in Dropbox folder (recursively, including subfolders)."""
    if not dbx:
        print("‚ùå Dropbox token missing. Cannot fetch PDFs.")
        return []

    print(f" Scanning Dropbox folder: {folder_path}")
    pdf_files = []

    try:
        result = dbx.files_list_folder(folder_path, recursive=True)
    except dropbox.exceptions.ApiError as e:
        print("Dropbox API error:", e)
        return pdf_files

    # Process first batch
    for entry in result.entries:
        if isinstance(entry, dropbox.files.FileMetadata) and entry.name.lower().endswith(".pdf"):
            pdf_files.append(entry)

    # Continue paging if there are more results
    while result.has_more:
        result = dbx.files_list_folder_continue(result.cursor)
        for entry in result.entries:
            if isinstance(entry, dropbox.files.FileMetadata) and entry.name.lower().endswith(".pdf"):
                pdf_files.append(entry)

    print(f" Found {len(pdf_files)} PDF(s).")
    return pdf_files

def extract_text_from_pdf(file_metadata):
    """Download and extract text from a Dropbox PDF"""
    if not dbx:
        return ""
        
    _, res = dbx.files_download(file_metadata.path_lower)
    pdf_bytes = BytesIO(res.content)
    with fitz.open(stream=pdf_bytes, filetype="pdf") as doc:
        text = "".join(page.get_text() for page in doc)
    return text

def chunk_text(text, size=500):
    """Split text into smaller chunks"""
    return [text[i:i+size] for i in range(0, len(text), size)]

def get_all_ids(collection):
    """Fetch all existing document IDs from ChromaDB."""
    all_ids = []
    offset = 0
    batch_size = 100
    while True:
        result = collection.get(limit=batch_size, offset=offset)
        if not result["ids"]:
            break
        all_ids.extend(result["ids"])
        offset += batch_size
    return set(all_ids)

def index_pdfs_if_new(collection, processing_status):
    """Index only new or modified PDFs."""
    if not dbx:
        print("‚ö†Ô∏è Skipping Dropbox indexing: No access token.")
        return

    processing_status["is_processing"] = True
    pdf_files = fetch_dropbox_pdfs()

    # ‚úÖ Correctly fetch existing IDs
    existing_ids = get_all_ids(collection)
    print("length of ids: ", len(existing_ids))
    skipped = 0
    
    for file_metadata in pdf_files:
        pdf_id = file_metadata.id or file_metadata.name

        # ‚úÖ Check if this file (by name) already exists in DB
        existing = collection.get(where={"source": file_metadata.name})
        if existing["metadatas"]:
            existing_rev = existing["metadatas"][0].get("rev")
            # Skip if same revision (unchanged file)
            if existing_rev == file_metadata.rev:
                print(f"‚è≠Ô∏è Skipping {file_metadata.name} ‚Äî no changes detected.")
                continue

        # ‚úÖ Skip if already indexed and no revision check available
        if pdf_id in existing_ids:
            print(f"‚è≠Ô∏è Already indexed: {file_metadata.name}")
            continue

        # Otherwise, process and index
        print(f"üìÑ Processing: {file_metadata.name}")
        try:
            text = extract_text_from_pdf(file_metadata)
            chunks = chunk_text(text)

            if not chunks:
                print(f"‚ö†Ô∏è Skipping {file_metadata.name} ‚Äî no text extracted.")
                skipped += 1
                continue

            base_id = pdf_id if pdf_id else file_metadata.name.replace(" ", "_").replace(".pdf", "")
            ids = [f"{base_id}_chunk{i}" for i in range(len(chunks))]
            ids = [i if i else f"{base_id}_chunk{idx}" for idx, i in enumerate(ids)]

            # ‚úÖ Include revision info in metadata
            collection.add(
                ids=ids,
                documents=chunks,
                metadatas=[{"source": file_metadata.name, "rev": file_metadata.rev}] * len(chunks)
            )

            print(f"‚úÖ Indexed {len(chunks)} chunks from {file_metadata.name}")

        except Exception as e:
            print(f"‚ùå Failed to process {file_metadata.name}: {e}")

    processing_status["is_processing"] = False
    processing_status["is_ready"] = True
    print("üìö All Dropbox PDFs processed and stored in ChromaDB!")
    print(f"Skipped {skipped} files")
